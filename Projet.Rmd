---
title: "Les pays à aider"
subtitle: | 
    | Projet de apprentissage non suppervisé
    | Classification
    | Université de Rennes II : Master Mathématiques Appliquées, Statistiques 
author: | 
    | Margaux Bailleul
    | Oriane Duclos
    

date: "`r format(Sys.time(), '%d %B, %Y')`"
geometry: "left=2.5cm,right=2.5cm,top=2cm,bottom=2cm"
fontfamily: mathpazo
output:
  pdf_document:
    keep_tex: true
    latex_engine: pdflatex
    fig_caption: yes
    highlight: haddock
    number_sections: yes
    toc: yes
    toc_depth: 2
    citation_package: natbib

---

# Chargement des librairies

```{r}
library(corrplot)
library(cluster)
library(NbClust)
library(FactoMineR)
library(ade4)

```

# 1 - Compréhension et pré-traitement des données

```{r}
donnee <- read.csv("Pays_donnees.csv", sep = ',', row.names = 1)
head(donnee,5)
```

```{r}
str(donnee)
dim(donnee)
```

Nous avons 167 individus et 9 variables

## Statistiques descriptives

```{r}
summary(donnee)
```

```{r eval=FALSE, include=FALSE}
# Histogramme de chaque variable
par(mfrow=c(3,3)) # Afficher les 9 histogrammes dans une grille 3x3
for (i in 1:9) {
  hist(donnee[,i], main=colnames(donnee)[i], xlab="")
}
```

## Pre-traimement

**Donnees manquantes ? Outliers**

```{r}
table(is.na(donnee))
```

Aucune donnée manquante

**Valeur aberrante** exports max à 200 ? Bizarre Ce sont à première vue
des pays riche comme malte, luxembourg, singapour import max à 174 ?
Idem Finalement c'est logique Aucune valeur aberrante

Mais y a des valeurs "leviers", certains pays comme malte, singapour se
dégage des valeurs moyennes

**Standardisation ?**

Lorsque l'on a des données avec des unités différentes (par exemple des
pourcentages, des espérances de vie, des PIB par habitant), il est
recommandé de centrer et de réduire ces données. Centrer les données
signifie soustraire la moyenne de la variable de toutes les
observations, ce qui permet d'avoir une moyenne égale à zéro. Réduire
les données signifie diviser chaque observation par l'écart-type de la
variable, ce qui met toutes les variables à la même échelle. Cela
facilite la comparaison entre les différentes variables et permet des
analyses statistiques plus fiables. Il est cependant important de garder
à l'esprit que la signification des résultats dépend toujours du
contexte et de la validité des données utilisées

```{r}
donnee <- data.frame(scale(donnee))
```

**Choix des variables (regroupement ?) en vue d'une classification**

## Matrice de corrélation

```{r}
var <- donnee[,1:9]
corrplot(cor(var), type = "upper")
```

La matrice de corrélation nous aide à mieux comprendre les relations
entre chaque variable et pourra nous aider à interpréter plus tard.

# 2 - Classification des pays en utilisant les différents algorithmes abordés en cours

Utilisation des algorithmes de classification vus en cours . Reflexion
sur les choix operer decider d'une classification finale . Nombre
de groupes ?

### Partie 1 : Algoritlme des Kmeans

Tout d'abord nous allons utiliser l'algorithme des k-means pour avoir
une première idée de notre classification finale. Si on ne sait pas a
priori combien de groupes comporte le jeu de donnees, on peut appliquer
l'algorithme pour plusieurs choix de K possibles et tracer la courbe
d'évolution de l'inertie . On lance l'algorithme des kmeans et on
observe l'évolution de la variance intra-groupes en fonction du nombre
de groupes. On rajoute également l'option « nstart =50 » pour stabiliser
les résultats.

```{r,echo=FALSE}
set.seed(123)
c <-  sapply(1:10,FUN=function(k){ kmeans(donnee,k)$tot.withinss })
plot(c,type="b")
```

A la vue de ce graphique, on aurait tendance à choisir K= 3,4 ou 5
groupes en appliquant la méthode dite « du coude »

```{r}
K=4
cl = kmeans(donnee,K,nstart=50)
gpe = cl$cluster
clusplot(donnee,gpe,labels=4,col.p=gpe)

```

La représentation en clusplot nous permet de voir qu'il y a 4 groupes
qui se séparent plutôt bien sur les composante 1, 2, 3 et 4. (on le voit
au travers des différents couleur sur le graphique).

Representation des groupes sur le premier plan factoriel

```{r}

```

### Partie 2 : CAH

```{r}
set.seed(123)
d <- dist(donnee)
#d <- dist(e19, method = "manhattan")
#d <- dist(e19, method = "minkowski")
cah.ward <- hclust(d, method = "ward.D")
cah.min <- hclust(d, method = "single")
cah.max <- hclust(d, method = "complete")
```

**Dengrogrammes**

```{r}
plot(cah.ward, hang = -1, main = "Distance de Ward", ylab = " ")
```

```{r}
plot(cah.min, hang = -1, main = "Distance du saut minimal", ylab = " ")
```

```{r}
plot(cah.max, hang = -1, main = "Distance du saut maximal", ylab = " ")
```

On s'apercoit raipdement que c'est le critère de Ward qui correspond le
mieux à nos données. On voit déjà qu'on peut partitionner nos données en
3 ou 4 groupes

**Fonction de perte**

Pour rappel, on cherche à maximiser l'inertie inter-classe. En effet,
nous avons pour objectif de créer des groupes d'individus se ressemblant
fortement (inertie intra-classes faible) et tels que les groupes soient
les plus distints possible (inertie inter-classes élevée). L'inertie
inter-classe est logiquement maximale (égale à l'intertie totale)
lorsqu'il y a autant de classes que d'individus. Nous cherchons dans le
graphique ci-dessous un "coude" qui correspond à une rupture dans la
courbe (moment où l'inertie inter augmente beaucoup).

```{r}
plot(rev(cah.ward$height)[1:10], type = "b", main = "Distance de Ward")
```

```{r}
plot(rev(cah.min$height)[1:10], type = "b", main = "Distance du saut minimal")
```

```{r}
plot(rev(cah.max$height)[1:10], type = "b", main = "Distance du saut maximal")
```

Avec le critère de Ward, la trace de la perte d'inertie nous incite à
choisir des partitions en 3 groupes ("coude" très visible).

```{r}
as.matrix(donnee)
```

```{r}
NbClust(as.matrix(donnee), min.nc = 2, max.nc = 15, method = "ward.D")
```

Voir quand on knit en html pour l'interprétation

**Cutree**

```{r}
nbc <- 3
gpe.ward <- cutree(cah.ward, k = nbc) # Classe affectée pour chaque individu
gpe.min <- cutree(cah.min, k = nbc)
gpe.max <- cutree(cah.max, k = nbc)
plot(cah.ward, hang = -1, main = "Distance de Ward")
rect.hclust(cah.ward, nbc, border = "blue")
```

```{r}
plot(cah.min, hang = -1, main = "Distance du saut minimal")
rect.hclust(cah.min, nbc, border = "blue")
```

```{r}
plot(cah.max, hang = -1, main = "Distance du saut maximal")
rect.hclust(cah.max, nbc, border = "blue")
```

```{r}
clusplot(donnee, gpe.ward, labels = nbc, col.p = as.numeric(gpe.ward))
```

Nous allons maintenant chercher à interpréter les groupes obtenus à
l'aide de la fonction catdes.

```{r}
gpe = cutree(cah.ward,k=3)
donnee$gpecah = as.factor(gpe)
interpcah = catdes(donnee,num.var = 10)
interpcah
head(donnee)
```

```{r}
plot.catdes(interpcah,barplot=T)
```

Les 3 groupes sont donc caractérisés ainsi :

-   Le premier groupe a une très faible espérance de vie, un faible
    revenu, un faible pib, et un fort taux de fertilité et de mortalité
    infantile.

-   Le second groupe se démarque déjà très largement du premier. En
    effet, il a un faible taux de mort infantile et une haute espérance
    de vie. Il a cependant un pib par habitant plutôt faible, mais
    toujours moins que le premier groupe.

-   Le troisième groupe se démarque également du deuxième groupe : il a
    un très fort pib par habitant, de forts revenus.
